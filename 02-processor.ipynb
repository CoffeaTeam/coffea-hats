{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coffea Processors\n",
    "\n",
    "Coffea relies mainly on [uproot](https://github.com/scikit-hep/uproot) to provide access to ROOT files for analysis.\n",
    "As a usual analysis will involve processing tens to thousands of files, totalling gigabytes to terabytes of data, there is a certain amount of work to be done to build a parallelized framework to process the data in a reasonable amount of time. Of course, one can work directly within uproot to achieve this, as we'll show in the beginning, but coffea provides the `coffea.processor` module, which allows users to worry just about the actual analysis code and not about how to implement efficient parallelization, assuming that the parallization is a trivial map-reduce operation (e.g. filling histograms and adding them together). The module provides the following key features:\n",
    "\n",
    " * A `ProcessorABC` abstract base class that can be derived from to implement the analysis code;\n",
    " * An interface to the arrays being read from the TTree, either [DataFrame](https://coffeateam.github.io/coffea/api/coffea.processor.LazyDataFrame.html#coffea.processor.LazyDataFrame) or [NanoEvents](https://coffeateam.github.io/coffea/api/coffea.nanoaod.NanoEvents.html#coffea.nanoaod.NanoEvents), to be used as inputs;\n",
    " * A set of accumulator types (value_accumulator, list_accumulator, set_accumulator, dict_accumulator, defaultdict_accumulator, column_accumulator) as described further [here](https://coffeateam.github.io/coffea/modules/coffea.processor.html#classes) to be used as output; and\n",
    " * A set of parallel executors to access multicore processing or distributed computing systems such as [Dask](https://distributed.dask.org/en/latest/), [Parsl](http://parsl-project.org/), [Spark](https://spark.apache.org/), and others.\n",
    "\n",
    "Let's start by writing a simple processor class that reads some CMS open data and plots a dimuon mass spectrum.\n",
    "We'll start by copying the [ProcessorABC](https://coffeateam.github.io/coffea/api/coffea.processor.ProcessorABC.html#coffea.processor.ProcessorABC) skeleton and filling in some details:\n",
    "\n",
    " * Remove `flag`, as we won't use it\n",
    " * Adding a new histogram for $m_{\\mu \\mu}$\n",
    " * Building a [JaggedCandidateArray](https://coffeateam.github.io/coffea/api/coffea.analysis_objects.JaggedCandidateMethods.html#coffea.analysis_objects.JaggedCandidateMethods.candidatesfromcounts) for muons,\n",
    " * Calculating the dimuon invariant mass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from coffea import hist, processor\n",
    "from coffea.analysis_objects import JaggedCandidateArray\n",
    "\n",
    "class MyProcessor(processor.ProcessorABC):\n",
    "    def __init__(self):\n",
    "        self._accumulator = processor.dict_accumulator({\n",
    "            \"sumw\": processor.defaultdict_accumulator(float),\n",
    "            \"mass\": hist.Hist(\n",
    "                \"Events\",\n",
    "                hist.Cat(\"dataset\", \"Dataset\"),\n",
    "                hist.Bin(\"mass\", \"$m_{\\mu\\mu}$ [GeV]\", 60, 60, 120),\n",
    "            ),\n",
    "        })\n",
    "\n",
    "    @property\n",
    "    def accumulator(self):\n",
    "        return self._accumulator\n",
    "\n",
    "    def process(self, df):\n",
    "        output = self.accumulator.identity()\n",
    "\n",
    "        dataset = df['dataset']\n",
    "        muons = JaggedCandidateArray.candidatesfromcounts(\n",
    "            df['nMuon'],\n",
    "            pt=df['Muon_pt'].content,\n",
    "            eta=df['Muon_eta'].content,\n",
    "            phi=df['Muon_phi'].content,\n",
    "            mass=df['Muon_mass'].content,\n",
    "            charge=df['Muon_charge'].content,\n",
    "        )\n",
    "\n",
    "        cut = (muons.counts == 2) & (muons['charge'].prod() == -1)        \n",
    "        dimuons = muons[cut].choose(2)\n",
    "        \n",
    "        output[\"sumw\"][dataset] += df.size\n",
    "        output[\"mass\"].fill(\n",
    "            dataset=dataset,\n",
    "            mass=dimuons.mass.flatten(),\n",
    "        )\n",
    "\n",
    "        return output\n",
    "\n",
    "    def postprocess(self, accumulator):\n",
    "        return accumulator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we were to just use bare uproot to execute this processor, we could do that with the following example, which:\n",
    "\n",
    " * Opens a CMS open data file\n",
    " * Creates a lazy data frame (roughly equivalent to [uproot.lazyarrays](https://uproot.readthedocs.io/en/latest/opening-files.html#uproot.tree.lazyarrays) but with some specializations needed for othere execution environments)\n",
    " * Creates a `MyProcessor` instance\n",
    " * Runs the `process()` function, which returns our accumulators\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uproot\n",
    "\n",
    "filename = \"root://eospublic.cern.ch//eos/root-eos/cms_opendata_2012_nanoaod/Run2012B_DoubleMuParked.root\"\n",
    "file = uproot.open(filename)\n",
    "df = processor.LazyDataFrame(\n",
    "    tree=file[\"Events\"],\n",
    "    entrystart=0,\n",
    "    entrystop=10000,\n",
    ")\n",
    "df[\"dataset\"] = \"DoubleMuon\"\n",
    "p = MyProcessor()\n",
    "out = p.process(df)\n",
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One could expand on this code to run over several chunks of the file, setting `entrystart` and `entrystop` as appropriate. Then, several datasets could be processed by iterating over several files. However, [run_uproot_job](https://coffeateam.github.io/coffea/api/coffea.processor.run_uproot_job.html#coffea.processor.run_uproot_job) can help with this. One lists the datasets and corresponding files, the processor they want to run, and which executor they want to use. Available executors are listed [here](https://coffeateam.github.io/coffea/modules/coffea.processor.html#functions). Since these files are very large, we limit to just reading the first few chunks of events from each dataset with `maxchunks`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileset = {\n",
    "    'DoubleMuon': [\n",
    "        'root://eospublic.cern.ch//eos/root-eos/cms_opendata_2012_nanoaod/Run2012B_DoubleMuParked.root',\n",
    "        'root://eospublic.cern.ch//eos/root-eos/cms_opendata_2012_nanoaod/Run2012C_DoubleMuParked.root',\n",
    "    ],\n",
    "    'ZZ to 4mu': [\n",
    "        'root://eospublic.cern.ch//eos/root-eos/cms_opendata_2012_nanoaod/ZZTo4mu.root'\n",
    "    ]\n",
    "}\n",
    "\n",
    "out = processor.run_uproot_job(\n",
    "    fileset,\n",
    "    treename=\"Events\",\n",
    "    processor_instance=MyProcessor(),\n",
    "    executor=processor.iterative_executor,\n",
    "    maxchunks=4,\n",
    ")\n",
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, if we want to use more than a single core on our machine, we simply change [iterative_executor](https://coffeateam.github.io/coffea/api/coffea.processor.iterative_executor.html) for [futures_executor](https://coffeateam.github.io/coffea/api/coffea.processor.futures_executor.html), which uses the python [concurrent.futures](https://docs.python.org/3/library/concurrent.futures.html) standard library. Optional arguments to these executors can be provided via `executor_args` parameter of `run_uproot_job`, such as the number of cores to use (2):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = processor.run_uproot_job(\n",
    "    fileset,\n",
    "    treename=\"Events\",\n",
    "    processor_instance=MyProcessor(),\n",
    "    executor=processor.futures_executor,\n",
    "    executor_args={'workers': 2},\n",
    "    maxchunks=4,\n",
    ")\n",
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hopefully this ran faster than the previous cell, but that may depend on how many cores are available on the machine you are running this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting fancy\n",
    "Let's flesh out this analysis into a 4-muon analysis, searching for diboson events:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import awkward as ak\n",
    "\n",
    "class FancyDimuonProcessor(processor.ProcessorABC):\n",
    "    def __init__(self):\n",
    "        dataset_axis = hist.Cat(\"dataset\", \"Primary dataset\")\n",
    "        mass_axis = hist.Bin(\"mass\", r\"$m_{\\mu\\mu}$ [GeV]\", 600, 0.25, 300)\n",
    "        pt_axis = hist.Bin(\"pt\", r\"$p_{T,\\mu}$ [GeV]\", 3000, 0.25, 300)\n",
    "        \n",
    "        self._accumulator = processor.dict_accumulator({\n",
    "            'mass': hist.Hist(\"Counts\", dataset_axis, mass_axis),\n",
    "            'mass_near': hist.Hist(\"Counts\", dataset_axis, mass_axis),\n",
    "            'mass_far': hist.Hist(\"Counts\", dataset_axis, mass_axis),\n",
    "            'pt_lead': hist.Hist(\"Counts\", dataset_axis, pt_axis),\n",
    "            'pt_trail': hist.Hist(\"Counts\", dataset_axis, pt_axis),\n",
    "            'cutflow': processor.defaultdict_accumulator(int),\n",
    "        })\n",
    "    \n",
    "    @property\n",
    "    def accumulator(self):\n",
    "        return self._accumulator\n",
    "    \n",
    "    def process(self, df):\n",
    "        output = self.accumulator.identity()\n",
    "        \n",
    "        dataset = df['dataset']\n",
    "        muons = JaggedCandidateArray.candidatesfromcounts(\n",
    "            df['nMuon'],\n",
    "            pt=df['Muon_pt'],\n",
    "            eta=df['Muon_eta'],\n",
    "            phi=df['Muon_phi'],\n",
    "            mass=df['Muon_mass'],\n",
    "            charge=df['Muon_charge'],\n",
    "            softId=df['Muon_softId'],\n",
    "            tightId=df['Muon_tightId']\n",
    "            )        \n",
    "        \n",
    "        output['cutflow']['all events'] += muons.size\n",
    "        \n",
    "        soft_id = (muons.softId > 0)\n",
    "        muons = muons[soft_id]\n",
    "        output['cutflow']['soft id'] += soft_id.any().sum()\n",
    "        \n",
    "        twomuons = (muons.counts >= 2)\n",
    "        output['cutflow']['two muons'] += twomuons.sum()\n",
    "        \n",
    "        dimuons = muons[twomuons].distincts()\n",
    "        \n",
    "        twodimuons = (dimuons.counts >= 2)\n",
    "        output['cutflow']['>= two dimuons'] += twodimuons.sum()\n",
    "        dimuons = dimuons[twodimuons]\n",
    "        \n",
    "        opposite_charge = (dimuons.i0['charge'] * dimuons.i1['charge'] == -1)\n",
    "        \n",
    "        dimuons = dimuons[opposite_charge]\n",
    "        output['cutflow']['opposite charge'] += opposite_charge.any().sum()\n",
    "        \n",
    "        mass_20GeV = (dimuons.mass > 35)\n",
    "        dimuons = dimuons[mass_20GeV]\n",
    "        \n",
    "        exactlytwodimuons = (dimuons.counts == 2)\n",
    "        output['cutflow']['== two dimuons'] += exactlytwodimuons.sum()\n",
    "        dimuons = dimuons[exactlytwodimuons].compact()\n",
    "        \n",
    "        leading_mu = (dimuons.i0.pt.content > dimuons.i1.pt.content)\n",
    "        pt_lead = ak.JaggedArray.fromoffsets(\n",
    "            dimuons.offsets,\n",
    "            np.where(leading_mu, dimuons.i0.pt.content, dimuons.i1.pt.content)\n",
    "        )\n",
    "        pt_trail = ak.JaggedArray.fromoffsets(\n",
    "            dimuons.offsets,\n",
    "            np.where(~leading_mu, dimuons.i0.pt.content, dimuons.i1.pt.content)\n",
    "        )\n",
    "        \n",
    "        near_z = np.abs(dimuons.mass - 91.118).argmin()\n",
    "        far_z = np.abs(dimuons.mass - 91.118).argmax()\n",
    "        \n",
    "        output['mass'].fill(dataset=dataset,\n",
    "                            mass=dimuons.p4.sum().mass)\n",
    "        output['mass_near'].fill(dataset=dataset, \n",
    "                                 mass=dimuons.mass[near_z].flatten())\n",
    "        output['mass_far'].fill(dataset=dataset, \n",
    "                                mass=dimuons.mass[far_z].flatten())\n",
    "        output['pt_lead'].fill(dataset=dataset,\n",
    "                               pt=pt_lead.flatten())\n",
    "        output['pt_trail'].fill(dataset=dataset,\n",
    "                                pt=pt_trail.flatten())\n",
    "        return output\n",
    "\n",
    "    def postprocess(self, accumulator):\n",
    "        return accumulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "tstart = time.time()    \n",
    "\n",
    "fileset = {\n",
    "    'DoubleMuon': [\n",
    "        'root://eospublic.cern.ch//eos/root-eos/cms_opendata_2012_nanoaod/Run2012B_DoubleMuParked.root',\n",
    "        'root://eospublic.cern.ch//eos/root-eos/cms_opendata_2012_nanoaod/Run2012C_DoubleMuParked.root',\n",
    "    ],\n",
    "    'ZZ to 4mu': [\n",
    "        'root://eospublic.cern.ch//eos/root-eos/cms_opendata_2012_nanoaod/ZZTo4mu.root'\n",
    "    ]\n",
    "}\n",
    "\n",
    "output = processor.run_uproot_job(\n",
    "    fileset,\n",
    "    treename='Events',\n",
    "    processor_instance=FancyDimuonProcessor(),\n",
    "    executor=processor.futures_executor,\n",
    "    executor_args={'workers': 6, 'flatten': True},\n",
    "    chunksize=100000,\n",
    "    maxchunks=10,\n",
    ")\n",
    "\n",
    "elapsed = time.time() - tstart\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What follows is just us looking at the output, you can execute it if you wish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "ax = hist.plot1d(output['mass'], overlay='dataset')\n",
    "ax.set_xlim(70,150)\n",
    "ax.set_ylim(0, 3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = hist.plot1d(output['mass_near'], overlay='dataset')\n",
    "ax.set_xlim(60,120)\n",
    "ax.set_ylim(0.1, 7500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = hist.plot1d(output['mass_far'], overlay='dataset')\n",
    "ax.set_yscale('log')\n",
    "ax.set_ylim(0.1, 8000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = hist.plot1d(output['pt_lead'], overlay='dataset')\n",
    "ax.set_yscale('log')\n",
    "ax.set_ylim(0.1, 5e3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = hist.plot1d(output['pt_trail'], overlay='dataset')\n",
    "ax.set_yscale('log')\n",
    "ax.set_ylim(0.1, 2e4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Events/s:\", output['cutflow']['all events']/elapsed)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "coffea-hats",
   "language": "python",
   "name": "coffea-hats"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
