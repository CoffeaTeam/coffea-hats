{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying corrections to columnar data\n",
    "\n",
    "Here we will show how to apply corrections to columnar data using:\n",
    "\n",
    "- the `coffea.lookup_tools` package, which is designed to read in ROOT histograms and a variety of data file formats popular within CMS into a standardized lookup table format;\n",
    "- CMS-specific extensions to the above, for jet corrections (`coffea.jetmet_tools`) and b-tagging efficiencies/uncertainties (`coffea.btag_tools`);\n",
    "- the [correctionlib](https://cms-nanoaod.github.io/correctionlib/) package, which provides a experiment-agnostic serializable data format for common correction functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test data**:\n",
    "We'll use NanoEvents to construct some test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import awkward as ak\n",
    "from coffea.nanoevents import NanoEventsFactory, NanoAODSchema\n",
    "\n",
    "fname = \"https://raw.githubusercontent.com/CoffeaTeam/coffea/master/tests/samples/nano_dy.root\"\n",
    "events = NanoEventsFactory.from_root(\n",
    "    fname,\n",
    "    schemaclass=NanoAODSchema.v6,\n",
    "    metadata={\"dataset\": \"DYJets\"},\n",
    ").events()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coffea lookup_tools\n",
    "\n",
    "The entrypoint for `coffea.lookup_tools` is the [extractor class](https://coffeateam.github.io/coffea/api/coffea.lookup_tools.extractor.html#coffea.lookup_tools.extractor)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from coffea.lookup_tools import extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# download some sample correction sources\n",
    "mkdir -p data\n",
    "pushd data\n",
    "PREFIX=https://raw.githubusercontent.com/CoffeaTeam/coffea/master/tests/samples\n",
    "curl -Os $PREFIX/testSF2d.histo.root\n",
    "curl -Os $PREFIX/Fall17_17Nov2017_V32_MC_L2Relative_AK4PFPuppi.jec.txt\n",
    "curl -Os $PREFIX/Fall17_17Nov2017_V32_MC_Uncertainty_AK4PFPuppi.junc.txt\n",
    "curl -Os $PREFIX/DeepCSV_102XSF_V1.btag.csv.gz\n",
    "popd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Opening a root file and using it as a lookup table\n",
    "\n",
    "In [tests/samples](https://github.com/CoffeaTeam/coffea/tree/master/tests/samples), there is an example file with a `TH2F` histogram named `scalefactors_Tight_Electron`. The following code reads that histogram into an [evaluator](https://coffeateam.github.io/coffea/api/coffea.lookup_tools.evaluator.html#coffea.lookup_tools.evaluator) instance, under the key `testSF2d` and applies it to some electrons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ext = extractor()\n",
    "# several histograms can be imported at once using wildcards (*)\n",
    "ext.add_weight_sets([\"testSF2d scalefactors_Tight_Electron data/testSF2d.histo.root\"])\n",
    "ext.finalize()\n",
    "\n",
    "evaluator = ext.make_evaluator()\n",
    "\n",
    "print(\"available evaluator keys:\")\n",
    "for key in evaluator.keys():\n",
    "    print(\"\\t\", key)\n",
    "print(\"testSF2d:\", evaluator['testSF2d'])\n",
    "print(\"type of testSF2d:\", type(evaluator['testSF2d']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Electron eta:\", events.Electron.eta)\n",
    "print(\"Electron pt:\", events.Electron.pt)\n",
    "print(\"Scale factor:\", evaluator[\"testSF2d\"](events.Electron.eta, events.Electron.pt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building and using your own correction from a histogram\n",
    "\n",
    "To use a histogram or ratio of histograms to build your own correction, you can use `lookup_tools` to simplify the implementation. Here we create some mock data for two slightly different pt and eta spectra (say, from two different generators) and derive a correction to reweight one sample to the other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import hist\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "dists = (\n",
    "    hist.Hist.new\n",
    "    .StrCat([\"gen1\", \"gen2\", \"gen2rwt\"], name=\"dataset\")\n",
    "    .Reg(20, 0, 100, name=\"pt\")\n",
    "    .Reg(4, -3, 3, name=\"eta\")\n",
    "    .Weight()\n",
    "    .fill(\n",
    "        dataset=\"gen1\",\n",
    "        pt=np.random.exponential(scale=10.0, size=10000) + np.random.exponential(scale=10.0, size=10000),\n",
    "        eta=np.random.normal(scale=1, size=10000)\n",
    "    )\n",
    "    .fill(\n",
    "        dataset=\"gen2\",\n",
    "        pt=np.random.exponential(scale=10.0, size=10000) + np.random.exponential(scale=15.0, size=10000),\n",
    "        eta=np.random.normal(scale=1.1, size=10000)\n",
    "    )\n",
    ")\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "dists[:, :, sum].plot1d(ax=ax)\n",
    "ax.legend(title=\"dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we derive a correction as a function of $p_T$ and $\\eta$ to `gen2` such that it agrees with `gen1`. We'll set it to 1 anywhere we run out of statistics for the correction, to avoid divide by zero issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from coffea.lookup_tools.dense_lookup import dense_lookup\n",
    "\n",
    "num = dists[\"gen1\", :, :].values()\n",
    "den = dists[\"gen2\", :, :].values()\n",
    "sf = np.where(\n",
    "    (num > 0) & (den > 0),\n",
    "    num / np.maximum(den, 1) * den.sum() / num.sum(),\n",
    "    1.0,\n",
    ")\n",
    "\n",
    "corr = dense_lookup(sf, [ax.edges for ax in dists.axes[1:]])\n",
    "print(corr)\n",
    "\n",
    "# a quick way to plot the scale factor is to steal the axis definitions from the input histograms:\n",
    "sfhist = hist.Hist(*dists.axes[1:], data=sf)\n",
    "sfhist.plot2d()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we generate some new mock data as if it was drawn from `gen2` and reweight it with our `corr` to match `gen1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ptvals = np.random.exponential(scale=10.0, size=10000) + np.random.exponential(scale=15.0, size=10000)\n",
    "etavals = np.random.normal(scale=1.1, size=10000)\n",
    "\n",
    "dists.fill(\n",
    "    dataset=\"gen2rwt\",\n",
    "    pt=ptvals,\n",
    "    eta=etavals,\n",
    "    weight=corr(ptvals, etavals)\n",
    ")\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "dists[:, :, sum].plot1d(ax=ax)\n",
    "ax.legend(title=\"dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that `corr()` can accept also jagged arrays if need be."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CMS high-level tools\n",
    "\n",
    "### Applying energy scale transformations with jetmet_tools\n",
    "\n",
    "The `coffea.jetmet_tools` package provides a convenience class [JetTransformer](https://coffeateam.github.io/coffea/api/coffea.jetmet_tools.JetTransformer.html#coffea.jetmet_tools.JetTransformer) which applies specified corrections and computes uncertainties in one call. First we build the desired jet correction stack to apply. This will usually be some set of the various JEC and JER correction text files that depends on the jet cone size (AK4, AK8) and the pileup mitigation algorithm, as well as the data-taking year they are associated with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from coffea.jetmet_tools import FactorizedJetCorrector, JetCorrectionUncertainty\n",
    "from coffea.jetmet_tools import JECStack, CorrectedJetsFactory\n",
    "import awkward as ak\n",
    "import numpy as np\n",
    "\n",
    "ext = extractor()\n",
    "ext.add_weight_sets([\n",
    "    \"* * data/Fall17_17Nov2017_V32_MC_L2Relative_AK4PFPuppi.jec.txt\",\n",
    "    \"* * data/Fall17_17Nov2017_V32_MC_Uncertainty_AK4PFPuppi.junc.txt\",\n",
    "])\n",
    "ext.finalize()\n",
    "\n",
    "jec_stack_names = [\n",
    "    \"Fall17_17Nov2017_V32_MC_L2Relative_AK4PFPuppi\",\n",
    "    \"Fall17_17Nov2017_V32_MC_Uncertainty_AK4PFPuppi\"\n",
    "]\n",
    "\n",
    "evaluator = ext.make_evaluator()\n",
    "\n",
    "jec_inputs = {name: evaluator[name] for name in jec_stack_names}\n",
    "jec_stack = JECStack(jec_inputs)\n",
    "### more possibilities are available if you send in more pieces of the JEC stack\n",
    "# mc2016_ak8_jxform = JECStack([\"more\", \"names\", \"of\", \"JEC parts\"])\n",
    "\n",
    "print(dir(evaluator))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we prepare some auxilary variables that are used to parameterize the jet energy corrections, such as jet area, mass, and event $\\rho$ (mean pileup energy density), and pass all of these into the `CorrectedJetsFactory`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_map = jec_stack.blank_name_map\n",
    "name_map['JetPt'] = 'pt'\n",
    "name_map['JetMass'] = 'mass'\n",
    "name_map['JetEta'] = 'eta'\n",
    "name_map['JetA'] = 'area'\n",
    "\n",
    "jets = events.Jet\n",
    "    \n",
    "jets['pt_raw'] = (1 - jets['rawFactor']) * jets['pt']\n",
    "jets['mass_raw'] = (1 - jets['rawFactor']) * jets['mass']\n",
    "jets['pt_gen'] = ak.values_astype(ak.fill_none(jets.matched_gen.pt, 0), np.float32)\n",
    "jets['rho'] = ak.broadcast_arrays(events.fixedGridRhoFastjetAll, jets.pt)[0]\n",
    "name_map['ptGenJet'] = 'pt_gen'\n",
    "name_map['ptRaw'] = 'pt_raw'\n",
    "name_map['massRaw'] = 'mass_raw'\n",
    "name_map['Rho'] = 'rho'\n",
    "    \n",
    "events_cache = events.caches[0]\n",
    "corrector = FactorizedJetCorrector(\n",
    "    Fall17_17Nov2017_V32_MC_L2Relative_AK4PFPuppi=evaluator['Fall17_17Nov2017_V32_MC_L2Relative_AK4PFPuppi'],\n",
    ")\n",
    "uncertainties = JetCorrectionUncertainty(\n",
    "    Fall17_17Nov2017_V32_MC_Uncertainty_AK4PFPuppi=evaluator['Fall17_17Nov2017_V32_MC_Uncertainty_AK4PFPuppi']\n",
    ")\n",
    "\n",
    "jet_factory = CorrectedJetsFactory(name_map, jec_stack)\n",
    "corrected_jets = jet_factory.build(jets, lazy_cache=events_cache)\n",
    "\n",
    "print('starting columns:', set(ak.fields(jets)))\n",
    "print('new columns:', set(ak.fields(corrected_jets)) - set(ak.fields(jets)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we show that the corrected jets indeed have a different $p_T$ and mass than we started with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('untransformed pt ratios', jets.pt/jets.pt_raw)\n",
    "print('untransformed mass ratios', jets.mass/jets.mass_raw)\n",
    "\n",
    "print('transformed pt ratios', corrected_jets.pt/corrected_jets.pt_raw)\n",
    "print('transformed mass ratios', corrected_jets.mass/corrected_jets.mass_raw)\n",
    "\n",
    "print('JES UP pt ratio', corrected_jets.JES_jes.up.pt/corrected_jets.pt_raw)\n",
    "print('JES DOWN pt ratio', corrected_jets.JES_jes.down.pt/corrected_jets.pt_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying CMS b-tagging corrections with btag_tools\n",
    "The `coffea.btag_tools` module provides the high-level utility [BTagScaleFactor](https://coffeateam.github.io/coffea/api/coffea.btag_tools.BTagScaleFactor.html#coffea.btag_tools.BTagScaleFactor) which calculates per-jet weights for b-tagging as well as light flavor mis-tagging efficiencies. Uncertainties can be calculated as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from coffea.btag_tools import BTagScaleFactor\n",
    "\n",
    "btag_sf = BTagScaleFactor(\"data/DeepCSV_102XSF_V1.btag.csv.gz\", \"medium\")\n",
    "\n",
    "print(\"SF:\", btag_sf.eval(\"central\", events.Jet.hadronFlavour, abs(events.Jet.eta), events.Jet.pt))\n",
    "print(\"systematic +:\", btag_sf.eval(\"up\", events.Jet.hadronFlavour, abs(events.Jet.eta), events.Jet.pt))\n",
    "print(\"systematic -:\", btag_sf.eval(\"down\", events.Jet.hadronFlavour, abs(events.Jet.eta), events.Jet.pt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using correctionlib\n",
    "\n",
    "For the most part, using correctionlib is straightforward. We'll show here how to convert the custom correction we derived earlier (`corr`) into a correctionlib object, and save it in the json format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import correctionlib, rich\n",
    "import correctionlib.convert\n",
    "\n",
    "# without a name, the resulting object will fail validation\n",
    "sfhist.name = \"gen2_to_gen1\"\n",
    "sfhist.label = \"out\"\n",
    "clibcorr = correctionlib.convert.from_histogram(sfhist)\n",
    "clibcorr.description = \"Reweights gen2 to agree with gen1\"\n",
    "# set overflow bins behavior (default is to raise an error when out of bounds)\n",
    "clibcorr.data.flow = \"clamp\"\n",
    "\n",
    "cset = correctionlib.schemav2.CorrectionSet(\n",
    "    schema_version=2,\n",
    "    description=\"my custom corrections\",\n",
    "    corrections=[clibcorr],\n",
    ")\n",
    "rich.print(cset)\n",
    "\n",
    "with open(\"data/mycorrections.json\", \"w\") as fout:\n",
    "    fout.write(cset.json(exclude_unset=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use this new correction in a similar way to the original `corr()` object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ceval = cset.to_evaluator()\n",
    "\n",
    "ceval[\"gen2_to_gen1\"].evaluate(ptvals, etavals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the time of writing, correctionlib does not support jagged arrays. But we can work around this using awkward utilities `flatten` and `unflatten`, as shown below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def myJetSF(jets):\n",
    "    j, nj = ak.flatten(jets), ak.num(jets)\n",
    "    sf = ceval[\"gen2_to_gen1\"].evaluate(np.array(j.pt), np.array(j.eta))\n",
    "    return ak.unflatten(sf, nj)\n",
    "\n",
    "myJetSF(events.Jet)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
